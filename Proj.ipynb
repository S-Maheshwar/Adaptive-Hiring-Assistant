{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDrwoHVodl7M",
    "outputId": "fb9e8b61-c665-4e52-a676-104d15be3f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain_community)\n",
      "  Downloading langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.40)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.11.10)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.33.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-classic, langchain_community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.79\n",
      "    Uninstalling langchain-core-0.3.79:\n",
      "      Successfully uninstalled langchain-core-0.3.79\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.11\n",
      "    Uninstalling langchain-text-splitters-0.3.11:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.4 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-core-1.0.4 langchain-text-splitters-1.0.0 langchain_community-0.4.1 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "!pip install -q gradio PyPDF2 python-docx nest_asyncio requests \"pydantic==2.5.0\" langchain_community \"crewai[tools]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and dependency bootstrap\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import asyncio\n",
    "import requests\n",
    "import traceback\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# CRITICAL: Set multiple dummy keys BEFORE importing CrewAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-dummy-local-only\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:11434\"\n",
    "os.environ[\"LITELLM_LOG\"] = \"ERROR\"  # Suppress LiteLLM logs\n",
    "\n",
    "# -----------------------------\n",
    "# Install/Import dependencies with specific versions\n",
    "# -----------------------------\n",
    "def _ensure(pkg, import_name=None, version=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg.split('[')[0])\n",
    "    except Exception:\n",
    "        install_pkg = f\"{pkg}=={version}\" if version else pkg\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", install_pkg])\n",
    "        __import__(import_name or pkg.split('[')[0])\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "_ensure(\"gradio\")\n",
    "_ensure(\"PyPDF2\", \"PyPDF2\")\n",
    "_ensure(\"python-docx\", \"docx\")\n",
    "_ensure(\"nest_asyncio\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"pydantic\", version=\"2.5.0\")  # Specific version for compatibility\n",
    "_ensure(\"langchain_community\")\n",
    "_ensure(\"crewai[tools]\")  # Install with tools support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports after dependency bootstrap\n",
    "import gradio as gr\n",
    "import PyPDF2\n",
    "import docx\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Import CrewAI components AFTER environment setup\n",
    "try:\n",
    "    from crewai import Agent, Task, Crew, Process\n",
    "    from crewai.tools import tool\n",
    "    from langchain_community.llms import Ollama as LC_Ollama\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå CrewAI import error: {e}\")\n",
    "    print(\"Reinstalling crewai...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--force-reinstall\", \"crewai[tools]\"])\n",
    "    from crewai import Agent, Task, Crew, Process\n",
    "    from crewai.tools import tool\n",
    "    from langchain_community.llms import Ollama as LC_Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OLLAMA_BASE = os.environ.get(\"OLLAMA_BASE\", \"http://127.0.0.1:11434\")\n",
    "DEFAULT_MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct-q4_K_M\")\n",
    "\n",
    "MODEL_CHOICES = [\n",
    "    \"llama3.1:8b-instruct-q4_K_M\",\n",
    "    \"mistral:7b-instruct-v0.3-q5_0\",\n",
    "    \"llama3.1:8b-instruct\",\n",
    "    \"mistral:7b-instruct\",\n",
    "    \"gpt-oss:20b\",\n",
    "]\n",
    "\n",
    "MAX_INPUT_TOKENS    = 2000\n",
    "MAX_OUTPUT_TOKENS   = 800\n",
    "MAX_QUESTION_TOKENS = 600\n",
    "MAX_CONTEXT_TOKENS  = 1200\n",
    "\n",
    "CURRENT_CONTEXT = \"\"\n",
    "LAST_ANALYSIS = {\"jd\": \"\", \"insights\": \"\"}\n",
    "\n",
    "CHAT_AVAILABLE = None\n",
    "ACTIVE_MODEL   = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System helpers (Ollama daemon + model)\n",
    "def _run(cmd, check=True, env=None):\n",
    "    return subprocess.run(cmd, check=check, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env)\n",
    "\n",
    "def _binary_exists(name):\n",
    "    try:\n",
    "        _run([\"bash\", \"-lc\", f\"command -v {name}\"])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _install_ollama_cli():\n",
    "    _run([\"bash\", \"-lc\", \"curl -fsSL https://ollama.com/install.sh | sh\"], check=True)\n",
    "\n",
    "def _start_ollama_daemon():\n",
    "    env = os.environ.copy()\n",
    "    _run([\"bash\", \"-lc\", \"nohup ollama serve > /tmp/ollama.log 2>&1 &\"], check=False, env=env)\n",
    "\n",
    "def _wait_for_http(url, timeout=180, interval=3):\n",
    "    start = time.time()\n",
    "    last_err = None\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            r = requests.get(url, timeout=5)\n",
    "            if r.status_code == 200:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "        time.sleep(interval)\n",
    "    if last_err:\n",
    "        raise RuntimeError(f\"Timeout reaching {url}: {last_err}\")\n",
    "    return False\n",
    "\n",
    "def _list_models():\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_BASE}/api/tags\", timeout=10)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return [m.get(\"name\") for m in data.get(\"models\", []) if m.get(\"name\")]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _pull_model_blocking(tag):\n",
    "    try:\n",
    "        print(f\"‚¨áÔ∏è Pulling model: {tag}\")\n",
    "        env = os.environ.copy()\n",
    "        res = _run([\"bash\", \"-lc\", f\"ollama pull {tag}\"], check=True, env=env)\n",
    "        print(res.stdout.strip())\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Pull failed for {tag}:\\n{e.stdout}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Pull exception for {tag}: {e}\")\n",
    "        return False\n",
    "\n",
    "def _resolve_installed_tag(preferred):\n",
    "    models = _list_models()\n",
    "    if not models:\n",
    "        return None\n",
    "    if preferred in models:\n",
    "        return preferred\n",
    "    for cand in [\n",
    "        \"llama3.1:8b-instruct-q4_K_M\",\n",
    "        \"mistral:7b-instruct-v0.3-q5_0\",\n",
    "        \"llama3.1:8b-instruct\",\n",
    "        \"mistral:7b-instruct\",\n",
    "    ]:\n",
    "        if cand in models:\n",
    "            return cand\n",
    "    return models[0]\n",
    "\n",
    "def _ensure_ollama_ready():\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_BASE}/api/tags\", timeout=5)\n",
    "        if r.ok:\n",
    "            return\n",
    "        raise RuntimeError(f\"Ollama API status {r.status_code}\")\n",
    "    except Exception:\n",
    "        if not _binary_exists(\"ollama\"):\n",
    "            _install_ollama_cli()\n",
    "        _start_ollama_daemon()\n",
    "        _wait_for_http(f\"{OLLAMA_BASE}/api/tags\", timeout=180, interval=3)\n",
    "\n",
    "def _ensure_model_available(tag):\n",
    "    global ACTIVE_MODEL\n",
    "    models = _list_models()\n",
    "    if tag not in models:\n",
    "        if not _pull_model_blocking(tag):\n",
    "            ACTIVE_MODEL = _resolve_installed_tag(tag)\n",
    "            return\n",
    "    ACTIVE_MODEL = tag\n",
    "\n",
    "def _probe_chat_support():\n",
    "    global CHAT_AVAILABLE\n",
    "    try:\n",
    "        payload = {\"model\": ACTIVE_MODEL, \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}], \"stream\": False}\n",
    "        resp = requests.post(f\"{OLLAMA_BASE}/api/chat\", json=payload, timeout=15)\n",
    "        if resp.status_code == 404:\n",
    "            CHAT_AVAILABLE = False\n",
    "        else:\n",
    "            resp.raise_for_status()\n",
    "            CHAT_AVAILABLE = True\n",
    "    except requests.HTTPError as he:\n",
    "        if getattr(he.response, \"status_code\", None) == 404:\n",
    "            CHAT_AVAILABLE = False\n",
    "        else:\n",
    "            CHAT_AVAILABLE = True\n",
    "    except Exception:\n",
    "        CHAT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token helpers\n",
    "def limit_tokens_by_words(text: str, max_tokens: int) -> str:\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "    words = text.split()\n",
    "    if len(words) > max_tokens:\n",
    "        return \" \".join(words[:max_tokens]) + f\"\\n\\n[TRUNCATED: kept first {max_tokens} tokens]\"\n",
    "    return text\n",
    "\n",
    "def limit_input(text: str, cap: int) -> str:\n",
    "    return limit_tokens_by_words(text, cap)\n",
    "\n",
    "def limit_output(text: str, cap: int) -> str:\n",
    "    return limit_tokens_by_words(text, cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File extraction\n",
    "def _extract_pdf_text_from_bytes(pdf_bytes: bytes) -> str:\n",
    "    try:\n",
    "        reader = PyPDF2.PdfReader(BytesIO(pdf_bytes))\n",
    "        if getattr(reader, \"is_encrypted\", False):\n",
    "            return \"‚ùå ERROR: PDF is password protected.\"\n",
    "        text = []\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                t = page.extract_text() or \"\"\n",
    "                text.append(t)\n",
    "            except Exception:\n",
    "                return f\"‚ùå ERROR: Could not read page {i+1} of PDF.\"\n",
    "        final = \"\\n\".join(text).strip()\n",
    "        if not final:\n",
    "            return \"‚ùå ERROR: PDF appears empty or image-only.\"\n",
    "        return final\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå ERROR: Invalid PDF. Details: {e}\"\n",
    "\n",
    "def _extract_docx_text(path: str) -> str:\n",
    "    try:\n",
    "        d = docx.Document(path)\n",
    "        parts = []\n",
    "        for p in d.paragraphs:\n",
    "            if p.text.strip():\n",
    "                parts.append(p.text)\n",
    "        for table in d.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    if cell.text.strip():\n",
    "                        parts.append(cell.text)\n",
    "        text = \"\\n\".join(parts).strip()\n",
    "        if not text:\n",
    "            return \"‚ùå ERROR: DOCX appears empty.\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå ERROR: Cannot read DOCX. Details: {e}\"\n",
    "\n",
    "def _extract_txt_text(path: str) -> str:\n",
    "    encs = [\"utf-8\",\"latin-1\",\"cp1252\",\"iso-8859-1\"]\n",
    "    for enc in encs:\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc) as f:\n",
    "                txt = f.read().strip()\n",
    "                if txt:\n",
    "                    return txt\n",
    "        except UnicodeError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå ERROR: Cannot read TXT. Details: {e}\"\n",
    "    return \"‚ùå ERROR: Unsupported TXT encoding.\"\n",
    "\n",
    "def extract_text_from_uploaded_file(file_obj) -> str:\n",
    "    try:\n",
    "        path = getattr(file_obj, \"name\", None) or (file_obj if isinstance(file_obj, str) else None)\n",
    "        if not path or not os.path.exists(path):\n",
    "            return \"‚ùå ERROR: File not found.\"\n",
    "        size = os.path.getsize(path)\n",
    "        if size == 0:\n",
    "            return \"‚ùå ERROR: File is empty.\"\n",
    "        if size > 10 * 1024 * 1024:\n",
    "            return \"‚ùå ERROR: File too large (>10MB).\"\n",
    "        ext = Path(path).suffix.lower()\n",
    "        if ext == \".pdf\":\n",
    "            with open(path, \"rb\") as f:\n",
    "                return _extract_pdf_text_from_bytes(f.read())\n",
    "        if ext == \".docx\":\n",
    "            return _extract_docx_text(path)\n",
    "        if ext == \".txt\":\n",
    "            return _extract_txt_text(path)\n",
    "        return f\"‚ùå ERROR: Unsupported file type '{ext}'. Use PDF/DOCX/TXT.\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå ERROR: Failed to process file. Details: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring helper\n",
    "def extract_match_score(text: str) -> int:\n",
    "    patterns = [\n",
    "        r\"OVERALL MATCH SCORE:\\s*\\**(\\d+)\\**/100\",\n",
    "        r\"MATCH SCORE:\\s*\\**(\\d+)\\**/100\",\n",
    "        r\"SCORE:\\s*\\**(\\d+)\\**/100\",\n",
    "        r\"(\\d+)/100\"\n",
    "    ]\n",
    "    for p in patterns:\n",
    "        m = re.search(p, text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                n = int(m.group(1))\n",
    "                return max(0, min(100, n))\n",
    "            except:\n",
    "                pass\n",
    "    return 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama API helpers\n",
    "def _messages_to_prompt(messages):\n",
    "    parts = []\n",
    "    sys_msgs = [m[\"content\"] for m in messages if m.get(\"role\") == \"system\"]\n",
    "    if sys_msgs:\n",
    "        parts.append(\"System:\\n\" + \"\\n\".join(sys_msgs).strip())\n",
    "    for m in messages:\n",
    "        role = m.get(\"role\")\n",
    "        if role in (\"user\", \"assistant\"):\n",
    "            parts.append(f\"{role.capitalize()}:\\n{m.get('content','').strip()}\")\n",
    "    parts.append(\"Assistant:\")\n",
    "    return \"\\n\\n\".join(parts).strip()\n",
    "\n",
    "def ollama_generate(prompt, temperature=0.2, max_tokens=MAX_OUTPUT_TOKENS):\n",
    "    payload = {\n",
    "        \"model\": ACTIVE_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": float(temperature), \"num_predict\": int(max_tokens)}\n",
    "    }\n",
    "    resp = requests.post(f\"{OLLAMA_BASE}/api/generate\", json=payload, timeout=120)\n",
    "    if resp.status_code == 404:\n",
    "        raise requests.HTTPError(\"404 on /api/generate\", response=resp)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if isinstance(data, dict) and \"response\" in data:\n",
    "        return data[\"response\"]\n",
    "    return json.dumps(data, indent=2)\n",
    "\n",
    "def ollama_chat(messages, temperature=0.2, max_tokens=MAX_OUTPUT_TOKENS):\n",
    "    global CHAT_AVAILABLE\n",
    "    if CHAT_AVAILABLE is None:\n",
    "        _probe_chat_support()\n",
    "    if CHAT_AVAILABLE is False:\n",
    "        prompt = _messages_to_prompt(messages)\n",
    "        return ollama_generate(prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    payload = {\n",
    "        \"model\": ACTIVE_MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": float(temperature), \"num_predict\": int(max_tokens)}\n",
    "    }\n",
    "    resp = requests.post(f\"{OLLAMA_BASE}/api/chat\", json=payload, timeout=120)\n",
    "    if resp.status_code == 404:\n",
    "        CHAT_AVAILABLE = False\n",
    "        prompt = _messages_to_prompt(messages)\n",
    "        return ollama_generate(prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if isinstance(data, dict) and \"message\" in data and isinstance(data[\"message\"], dict) and \"content\" in data[\"message\"]:\n",
    "        return data[\"message\"][\"content\"]\n",
    "    if isinstance(data, dict) and \"response\" in data:\n",
    "        return data[\"response\"]\n",
    "    return json.dumps(data, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for analysis\n",
    "def make_jd_prompt(resume_text: str, jd_text: str) -> list:\n",
    "    sys_prompt = (\n",
    "        \"You are a brutally honest recruitment analyst. Analyze the resume against the job description and \"\n",
    "        \"produce a realistic, strict evaluation with clear scores and rationale.\"\n",
    "    )\n",
    "    user_prompt = f\"\"\"\n",
    "RESUME:\n",
    "{resume_text}\n",
    "\n",
    "JOB DESCRIPTION:\n",
    "{jd_text}\n",
    "\n",
    "Provide response in EXACTLY this format:\n",
    "\n",
    "**OVERALL MATCH SCORE: [X]/100** [star rating: ‚≠ê for <30, ‚≠ê‚≠ê for 30-50, ‚≠ê‚≠ê‚≠ê for 50-70, ‚≠ê‚≠ê‚≠ê‚≠ê for 70-85, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê for 85+]\n",
    "\n",
    "**TECHNICAL SKILLS ([X]/10):**\n",
    "‚úÖ Matched: [list]\n",
    "‚ùå Missing: [list]\n",
    "\n",
    "**EXPERIENCE LEVEL ([X]/10):**\n",
    "‚Ä¢ Years: [actual years]\n",
    "‚Ä¢ Required: [years required]\n",
    "‚Ä¢ Gap: [explain]\n",
    "\n",
    "**EDUCATION ([X]/10):**\n",
    "‚Ä¢ Match Level: [Perfect/Good/Fair/Poor]\n",
    "\n",
    "**SUMMARY:**\n",
    "[Short honest rationale. If junior applying to senior, score low. Skills mismatch -> low score.]\n",
    "\"\"\".strip()\n",
    "    return [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "def make_insight_prompt(resume_text: str, jd_result: str) -> list:\n",
    "    sys_prompt = \"You are a logical talent assessor. Your recommendation MUST be consistent with the JD match score.\"\n",
    "    user_prompt = f\"\"\"\n",
    "RESUME:\n",
    "{resume_text}\n",
    "\n",
    "JD MATCH RESULT:\n",
    "{jd_result}\n",
    "\n",
    "CRITICAL RULE: Your recommendation MUST match the score:\n",
    "- 0-30: HARD PASS\n",
    "- 31-50: MAYBE (major concerns)\n",
    "- 51-70: CAUTIOUS YES\n",
    "- 71-85: RECOMMEND\n",
    "- 86-100: STRONG RECOMMEND\n",
    "\n",
    "Provide:\n",
    "**RECOMMENDATION:** [HARD PASS/MAYBE/CAUTIOUS YES/RECOMMEND/STRONG RECOMMEND] - [one-line reason]\n",
    "**KEY RISKS:** [bulleted list]\n",
    "**INTERVIEW FOCUS:** [bulleted list]\n",
    "\"\"\".strip()\n",
    "    return [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "def make_hr_chat_prompt(question: str, context: str) -> list:\n",
    "    sys_prompt = \"You are an HR assistant who ONLY discusses the analyzed candidate. Refuse to answer any unrelated questions.\"\n",
    "    user_prompt = f\"\"\"\n",
    "CANDIDATE ANALYSIS CONTEXT:\n",
    "{context}\n",
    "\n",
    "HR QUESTION:\n",
    "{question}\n",
    "\n",
    "RULES:\n",
    "1) ONLY answer about THIS candidate\n",
    "2) Stay consistent with analysis scores\n",
    "3) If match score is low (0-30), do not recommend hiring\n",
    "4) If there are experience gaps, acknowledge them\n",
    "5) NEVER answer general knowledge or unrelated questions\n",
    "\"\"\".strip()\n",
    "    return [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": user_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrewAI Tool Definitions (FIXED)\n",
    "@tool(\"JD Match Analyzer\")\n",
    "def jd_match_tool_crewai(resume_text: str, job_description: str) -> str:\n",
    "    \"\"\"Analyzes resume against job description with brutal honesty. Returns compatibility score and detailed breakdown.\"\"\"\n",
    "    try:\n",
    "        r = limit_input(resume_text, MAX_INPUT_TOKENS)\n",
    "        j = limit_input(job_description, MAX_INPUT_TOKENS)\n",
    "        out = ollama_chat(make_jd_prompt(r, j), temperature=0.2, max_tokens=MAX_OUTPUT_TOKENS)\n",
    "        return limit_output(out, MAX_OUTPUT_TOKENS)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "@tool(\"Critical Insight Generator\")\n",
    "def critical_insight_tool_crewai(resume_text: str, jd_match_result: str) -> str:\n",
    "    \"\"\"Generates critical insights consistent with JD match score. Returns recommendation aligned with score.\"\"\"\n",
    "    try:\n",
    "        r = limit_input(resume_text, MAX_INPUT_TOKENS)\n",
    "        d = limit_input(jd_match_result, MAX_INPUT_TOKENS)\n",
    "        out = ollama_chat(make_insight_prompt(r, d), temperature=0.2, max_tokens=MAX_OUTPUT_TOKENS)\n",
    "        return limit_output(out, MAX_OUTPUT_TOKENS)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "@tool(\"HR Chat Assistant\")\n",
    "def hr_chat_tool_crewai(question: str, context: str) -> str:\n",
    "    \"\"\"Answers HR questions about the analyzed candidate ONLY. Refuses unrelated questions.\"\"\"\n",
    "    try:\n",
    "        if not is_candidate_related_question(question):\n",
    "            return (\n",
    "                \"üö´ I can only answer questions about the analyzed candidate.\\n\\n\"\n",
    "                \"Please ask about hiring recommendation, strengths/weaknesses, score rationale, interview focus, or training needs.\"\n",
    "            )\n",
    "        q = limit_input(question, MAX_QUESTION_TOKENS)\n",
    "        c = limit_input(context, MAX_CONTEXT_TOKENS)\n",
    "        out = ollama_chat(make_hr_chat_prompt(q, c), temperature=0.2, max_tokens=MAX_OUTPUT_TOKENS)\n",
    "        return limit_output(out, MAX_OUTPUT_TOKENS)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "def is_candidate_related_question(question: str) -> bool:\n",
    "    q = question.lower().strip()\n",
    "    unrelated = [\"weather\",\"recipe\",\"cooking\",\"food\",\"sports\",\"politics\",\"religion\",\"celebrity\",\"movie\",\"music\",\"game\",\"animal\",\"planet\",\"element\",\"chemistry\",\"physics\",\"math\",\"history\",\"geography\",\"joke\",\"story\",\"define\",\"explain\",\"tell me about\"]\n",
    "    if any(k in q for k in unrelated):\n",
    "        return False\n",
    "    related = [\"candidate\",\"resume\",\"hire\",\"interview\",\"skill\",\"experience\",\"qualification\",\"score\",\"match\",\"recommend\",\"suitable\",\"fit\",\"strength\",\"weakness\",\"concern\",\"red flag\",\"education\",\"background\",\"position\",\"role\",\"job\",\"applicant\",\"talent\",\"employee\",\"work\",\"career\",\"professional\",\"technical\",\"training\",\"development\"]\n",
    "    if any(k in q for k in related):\n",
    "        return True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrewAI LLM and Agents (FIXED)\n",
    "def get_crewai_llm():\n",
    "    \"\"\"Returns LangChain Ollama LLM configured for current model\"\"\"\n",
    "    model_tag = ACTIVE_MODEL or DEFAULT_MODEL\n",
    "    return LC_Ollama(\n",
    "        base_url=OLLAMA_BASE,\n",
    "        model=model_tag,\n",
    "        temperature=0.2,\n",
    "        num_predict=MAX_OUTPUT_TOKENS\n",
    "    )\n",
    "\n",
    "def build_crewai_agents():\n",
    "    \"\"\"Build CrewAI agents with proper LLM binding\"\"\"\n",
    "    crew_llm = get_crewai_llm()\n",
    "\n",
    "    jd_agent = Agent(\n",
    "        role=\"Brutally Honest Job Matching Specialist\",\n",
    "        goal=\"Provide realistic compatibility scores with no sugar-coating\",\n",
    "        backstory=\"A no-nonsense recruitment analyst who gives honest scores.\",\n",
    "        verbose=False,\n",
    "        allow_delegation=False,\n",
    "        llm=crew_llm,\n",
    "        tools=[jd_match_tool_crewai]\n",
    "    )\n",
    "\n",
    "    insight_agent = Agent(\n",
    "        role=\"Logically Consistent Career Assessor\",\n",
    "        goal=\"Generate insights that match the compatibility scores with no contradictions\",\n",
    "        backstory=\"A logical talent assessor ensuring recommendations align with scores.\",\n",
    "        verbose=False,\n",
    "        allow_delegation=False,\n",
    "        llm=crew_llm,\n",
    "        tools=[critical_insight_tool_crewai]\n",
    "    )\n",
    "\n",
    "    hr_agent = Agent(\n",
    "        role=\"Focused HR Assistant (Candidate-only)\",\n",
    "        goal=\"ONLY answer questions about the analyzed candidate; refuse unrelated questions\",\n",
    "        backstory=\"A focused HR consultant who only discusses the analyzed candidate.\",\n",
    "        verbose=False,\n",
    "        allow_delegation=False,\n",
    "        llm=crew_llm,\n",
    "        tools=[hr_chat_tool_crewai]\n",
    "    )\n",
    "\n",
    "    return jd_agent, insight_agent, hr_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task Execution (FIXED)\n",
    "def crew_run_jd(jd_agent, resume_text, job_description):\n",
    "    \"\"\"Execute JD matching task\"\"\"\n",
    "    try:\n",
    "        # Direct tool call instead of complex crew setup\n",
    "        result = jd_match_tool_crewai.func(resume_text, job_description)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"ERROR in JD analysis: {str(e)}\\n{traceback.format_exc()}\"\n",
    "\n",
    "def crew_run_insight(insight_agent, resume_text, jd_result):\n",
    "    \"\"\"Execute insight generation task\"\"\"\n",
    "    try:\n",
    "        # Direct tool call instead of complex crew setup\n",
    "        result = critical_insight_tool_crewai.func(resume_text, jd_result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"ERROR in insight generation: {str(e)}\\n{traceback.format_exc()}\"\n",
    "\n",
    "def crew_run_hr(hr_agent, question, context):\n",
    "    \"\"\"Execute HR chat task\"\"\"\n",
    "    try:\n",
    "        # Direct tool call instead of complex crew setup\n",
    "        result = hr_chat_tool_crewai.func(question, context)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"ERROR in HR chat: {str(e)}\\n{traceback.format_exc()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio callbacks\n",
    "def cb_set_model(tag):\n",
    "    try:\n",
    "        _ensure_ollama_ready()\n",
    "        _ensure_model_available(tag or DEFAULT_MODEL)\n",
    "        _probe_chat_support()\n",
    "        return f\"‚úÖ Active model: {ACTIVE_MODEL}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå {e}\"\n",
    "\n",
    "def cb_analyze_resume(resume_file, job_description, selected_model):\n",
    "    try:\n",
    "        _ensure_ollama_ready()\n",
    "        _ensure_model_available(selected_model or DEFAULT_MODEL)\n",
    "        _probe_chat_support()\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå {e}\", \"\", \"\"\n",
    "\n",
    "    if not resume_file or not job_description or not str(job_description).strip():\n",
    "        return \"‚ùå Please upload a resume and provide a job description.\", \"\", \"\"\n",
    "\n",
    "    resume_text = extract_text_from_uploaded_file(resume_file)\n",
    "    if resume_text.startswith(\"‚ùå ERROR\"):\n",
    "        return resume_text, \"File processing failed.\", \"\"\n",
    "\n",
    "    try:\n",
    "        jd_agent, insight_agent, hr_agent = build_crewai_agents()\n",
    "        jd_res = crew_run_jd(jd_agent, resume_text, job_description)\n",
    "\n",
    "        if jd_res.startswith(\"ERROR\"):\n",
    "            return jd_res, \"Analysis failed.\", \"\"\n",
    "\n",
    "        insights = crew_run_insight(insight_agent, resume_text, jd_res)\n",
    "\n",
    "        if insights.startswith(\"ERROR\"):\n",
    "            return f\"{jd_res}\\n\\nInsight generation failed: {insights}\", \"Partial analysis complete.\", \"\"\n",
    "\n",
    "        global CURRENT_CONTEXT, LAST_ANALYSIS\n",
    "        combined = f\"JD MATCHING ANALYSIS:\\n{jd_res}\\n\\nCRITICAL INSIGHTS:\\n{insights}\"\n",
    "        CURRENT_CONTEXT = limit_input(combined, MAX_CONTEXT_TOKENS)\n",
    "        LAST_ANALYSIS = {\"jd\": jd_res, \"insights\": insights}\n",
    "\n",
    "        resume_tokens = len(resume_text.split())\n",
    "        jd_tokens = len(str(job_description).split())\n",
    "        out = (\n",
    "            f\"ü§ñ CrewAI Agents | ü¶ô Model: {ACTIVE_MODEL}\\n\"\n",
    "            f\"üìä (Resume: {resume_tokens} tokens, JD: {jd_tokens} tokens)\\n\\n\"\n",
    "            f\"üìä JD MATCHING ANALYSIS:\\n{jd_res}\\n\\n\"\n",
    "            f\"üìù CRITICAL INSIGHTS:\\n{insights}\"\n",
    "        )\n",
    "        return out, \"‚úÖ Analysis complete via CrewAI agents. You can now use the HR Chat tab.\", \"\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå CRITICAL ERROR: {e}\\n{traceback.format_exc()}\", \"\", \"\"\n",
    "\n",
    "def cb_hr_chat(question, selected_model):\n",
    "    if not CURRENT_CONTEXT:\n",
    "        return \"‚ö†Ô∏è Please analyze a candidate first in the Resume Analysis tab.\"\n",
    "    if not question or not str(question).strip():\n",
    "        return \"Please ask a question about the candidate.\"\n",
    "    try:\n",
    "        _ensure_ollama_ready()\n",
    "        _ensure_model_available(selected_model or DEFAULT_MODEL)\n",
    "        _probe_chat_support()\n",
    "        jd_agent, insight_agent, hr_agent = build_crewai_agents()\n",
    "        return crew_run_hr(hr_agent, question, CURRENT_CONTEXT)\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {e}\"\n",
    "\n",
    "def cb_batch_rank(resume_files, job_description, selected_model):\n",
    "    try:\n",
    "        _ensure_ollama_ready()\n",
    "        _ensure_model_available(selected_model or DEFAULT_MODEL)\n",
    "        _probe_chat_support()\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå {e}\", \"\"\n",
    "\n",
    "    if not resume_files or len(resume_files) == 0:\n",
    "        return \"‚ùå Please upload at least 2 resume files.\", \"\"\n",
    "    if len(resume_files) < 2:\n",
    "        return \"‚ùå Please upload at least 2 resumes for comparison.\", \"\"\n",
    "    if len(resume_files) > 5:\n",
    "        return \"‚ùå Maximum 5 resumes allowed.\", \"\"\n",
    "    if not job_description or not str(job_description).strip():\n",
    "        return \"‚ùå Please provide a job description.\", \"\"\n",
    "\n",
    "    jd_agent, insight_agent, hr_agent = build_crewai_agents()\n",
    "\n",
    "    results = []\n",
    "    for i, f in enumerate(resume_files, 1):\n",
    "        try:\n",
    "            text = extract_text_from_uploaded_file(f)\n",
    "            name = getattr(f, \"name\", f\"Resume_{i}\").split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "            if text.startswith(\"‚ùå ERROR\"):\n",
    "                results.append({\"rank\": None, \"file_name\": name, \"status\": \"failed\", \"score\": 0, \"jd\": text, \"insights\": \"\"})\n",
    "                continue\n",
    "            jd_res = crew_run_jd(jd_agent, text, job_description)\n",
    "            if jd_res.startswith(\"ERROR\"):\n",
    "                results.append({\"rank\": None, \"file_name\": name, \"status\": \"failed\", \"score\": 0, \"jd\": jd_res, \"insights\": \"\"})\n",
    "                continue\n",
    "            score = extract_match_score(jd_res)\n",
    "            insights = crew_run_insight(insight_agent, text, jd_res)\n",
    "            results.append({\"rank\": None, \"file_name\": name, \"status\": \"success\", \"score\": score, \"jd\": jd_res, \"insights\": insights})\n",
    "        except Exception as e:\n",
    "            name = getattr(f, \"name\", f\"Resume_{i}\").split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "            results.append({\"rank\": None, \"file_name\": name, \"status\": \"failed\", \"score\": 0, \"jd\": f\"‚ùå ERROR: {e}\", \"insights\": \"\"})\n",
    "\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    for rnk, item in enumerate(results, 1):\n",
    "        item[\"rank\"] = rnk\n",
    "\n",
    "    summary = (\n",
    "        \"üèÜ CANDIDATE RANKING SUMMARY\\n\"\n",
    "        f\"ü§ñ CrewAI Agents | ü¶ô Model: {ACTIVE_MODEL}\\n\"\n",
    "        f\"üìä Total Processed: {len(resume_files)}\\n\"\n",
    "        f\"‚úÖ Successfully Analyzed: {sum(1 for r in results if r['status']=='success')}\\n\"\n",
    "        f\"‚ùå Failed: {sum(1 for r in results if r['status']=='failed')}\\n\"\n",
    "    )\n",
    "\n",
    "    details = \"üèÜ DETAILED CANDIDATE RANKINGS\\n\\n\"\n",
    "    for c in results:\n",
    "        emoji = \"ü•á\" if c[\"rank\"] == 1 else \"ü•à\" if c[\"rank\"] == 2 else \"ü•â\" if c[\"rank\"] == 3 else f\"{c['rank']}Ô∏è‚É£\"\n",
    "        if c[\"score\"] >= 85:\n",
    "            stars = \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"; rec = \"STRONG RECOMMEND\"\n",
    "        elif c[\"score\"] >= 70:\n",
    "            stars = \"‚≠ê‚≠ê‚≠ê‚≠ê\"; rec = \"RECOMMEND\"\n",
    "        elif c[\"score\"] >= 50:\n",
    "            stars = \"‚≠ê‚≠ê‚≠ê\"; rec = \"CAUTIOUS YES\"\n",
    "        elif c[\"score\"] >= 30:\n",
    "            stars = \"‚≠ê‚≠ê\"; rec = \"MAYBE\"\n",
    "        else:\n",
    "            stars = \"‚≠ê\"; rec = \"HARD PASS\"\n",
    "        details += (\n",
    "            f\"{'='*80}\\n\"\n",
    "            f\"{emoji} RANK #{c['rank']} - {c['file_name']}\\n\"\n",
    "            f\"{'='*80}\\n\"\n",
    "            f\"üìä MATCH SCORE: {c['score']}/100 {stars}\\n\"\n",
    "            f\"üíº RECOMMENDATION: {rec}\\n\\n\"\n",
    "        )\n",
    "        if c[\"status\"] == \"success\":\n",
    "            details += f\"üìÑ ANALYSIS:\\n{c['jd']}\\n\\nüìù INSIGHTS:\\n{c['insights']}\\n\\n\"\n",
    "        else:\n",
    "            details += f\"‚ùå ERROR: {c['jd']}\\n\\n\"\n",
    "\n",
    "    return summary, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI\n",
    "def build_ui():\n",
    "    with gr.Blocks(title=\"Recruitment Portal (CrewAI + Ollama)\", theme=\"soft\", analytics_enabled=False) as demo:\n",
    "        gr.Markdown(\n",
    "            f\"\"\"\n",
    "# ü§ñ CrewAI Recruitment Portal (Fixed)\n",
    "## Multi-agent pipeline with local Ollama models\n",
    "\n",
    "- Agents: JD Matcher, Insight Generator, HR Chat\n",
    "- Default model: {DEFAULT_MODEL}\n",
    "- **Fixed**: LiteLLM errors, CrewAI tool integration, task execution\n",
    "            \"\"\".strip()\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            model_select = gr.Dropdown(choices=MODEL_CHOICES, value=DEFAULT_MODEL, label=\"Active Model\")\n",
    "            model_status = gr.Textbox(label=\"Model Status\", interactive=False, value=\"Ready\")\n",
    "        model_select.change(cb_set_model, inputs=[model_select], outputs=[model_status])\n",
    "\n",
    "        with gr.Tab(\"üìÑ Resume Analysis\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    resume_upload = gr.File(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Resume File (PDF/DOCX/TXT)\")\n",
    "                    job_description = gr.Textbox(lines=8, placeholder=\"Paste the job description here...\", label=\"Job Description\")\n",
    "                    analyze_button = gr.Button(\"ü§ñ Analyze with CrewAI\", variant=\"primary\")\n",
    "                with gr.Column(scale=2):\n",
    "                    analysis_output = gr.Textbox(lines=22, label=\"Analysis Output\", interactive=False)\n",
    "                    status_message = gr.Textbox(label=\"Status\", interactive=False, value=\"Ready\")\n",
    "            analyze_button.click(\n",
    "                cb_analyze_resume,\n",
    "                inputs=[resume_upload, job_description, model_select],\n",
    "                outputs=[analysis_output, status_message, gr.Textbox(visible=False)]\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"üí¨ HR Chat Assistant\"):\n",
    "            gr.Markdown(\"Ask questions about the analyzed candidate ONLY\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    hr_question = gr.Textbox(\n",
    "                        placeholder=\"Ask about hiring decision, risks, score rationale, interview focus...\",\n",
    "                        label=f\"Your Question (Max {MAX_QUESTION_TOKENS} tokens)\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    chat_button = gr.Button(\"üí¨ Ask\")\n",
    "                with gr.Column():\n",
    "                    chat_response = gr.Textbox(lines=14, label=f\"Response (Max {MAX_OUTPUT_TOKENS} tokens)\", interactive=False)\n",
    "            chat_button.click(cb_hr_chat, inputs=[hr_question, model_select], outputs=[chat_response])\n",
    "\n",
    "        with gr.Tab(\"üìä Batch Resume Ranking\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    batch_files = gr.File(file_count=\"multiple\", file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Upload Resumes (2-5 files)\")\n",
    "                    batch_jd = gr.Textbox(lines=8, placeholder=\"Paste the job description here...\", label=\"Job Description (Same for All)\")\n",
    "                    batch_btn = gr.Button(\"üèÜ Rank Candidates\", variant=\"primary\")\n",
    "                with gr.Column(scale=2):\n",
    "                    summary_box = gr.Textbox(lines=8, label=\"Summary\", interactive=False)\n",
    "                    ranking_box = gr.Textbox(lines=22, label=\"Detailed Rankings\", interactive=False)\n",
    "            batch_btn.click(cb_batch_rank, inputs=[batch_files, batch_jd, model_select], outputs=[summary_box, ranking_box])\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch (notebook-safe)\n",
    "def launch_app():\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_closed():\n",
    "            asyncio.set_event_loop(asyncio.new_event_loop())\n",
    "        nest_asyncio.apply()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    gr.close_all()\n",
    "    demo = build_ui()\n",
    "\n",
    "    in_colab = \"COLAB_RELEASE_TAG\" in os.environ or \"COLAB_GPU\" in os.environ\n",
    "\n",
    "    demo.queue()\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        share=True if in_colab else False,\n",
    "        inline=True,\n",
    "        debug=False,\n",
    "        prevent_thread_lock=True,\n",
    "        show_error=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _ensure_ollama_ready()\n",
    "    _ensure_model_available(DEFAULT_MODEL)\n",
    "    _probe_chat_support()\n",
    "    print(f\"‚úÖ Ready. Active model: {ACTIVE_MODEL} | Chat endpoint: {('yes' if CHAT_AVAILABLE else 'fallback')}\")\n",
    "    launch_app()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
